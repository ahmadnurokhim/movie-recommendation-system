{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ratings and movies data\n",
    "ratings = pd.read_csv(\"data/ratings_200k.csv\")\n",
    "movies = pd.read_csv('data/movies_2000.csv')\n",
    "\n",
    "# Get movie title to ratings DF\n",
    "ratings = pd.merge(ratings, movies[['movieId', 'title']], on='movieId')[['userId', 'title', 'rating']]\n",
    "\n",
    "# Convert value to byte\n",
    "ratings = ratings.apply(lambda x: x.apply(lambda y: str(y).encode()))\n",
    "\n",
    "# Convert DF to TF dataset\n",
    "ratings = tf.data.Dataset.from_tensor_slices(ratings)\n",
    "\n",
    "# Map so each row can be called by its name\n",
    "ratings = ratings.map(lambda x: {\n",
    "    'userId': x[0],\n",
    "    'title': x[1],\n",
    "    'rating': float(b'2.5')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m user_ids \u001b[39m=\u001b[39m ratings\u001b[39m.\u001b[39mbatch(\u001b[39m1000000\u001b[39m)\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     12\u001b[0m uniq_movie_title \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(np\u001b[39m.\u001b[39mconcatenate(\u001b[39mlist\u001b[39m(movie_titles)))\n\u001b[1;32m---> 13\u001b[0m uniq_user_id \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(np\u001b[39m.\u001b[39mconcatenate(\u001b[39mlist\u001b[39;49m(user_ids)))\n",
      "File \u001b[1;32md:\\Work\\movie-recommendation-system\\.venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:797\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    796\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 797\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_internal()\n\u001b[0;32m    798\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    799\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32md:\\Work\\movie-recommendation-system\\.venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:780\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[39m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[0;32m    778\u001b[0m \u001b[39m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[0;32m    779\u001b[0m \u001b[39mwith\u001b[39;00m context\u001b[39m.\u001b[39mexecution_mode(context\u001b[39m.\u001b[39mSYNC):\n\u001b[1;32m--> 780\u001b[0m   ret \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39;49miterator_get_next(\n\u001b[0;32m    781\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource,\n\u001b[0;32m    782\u001b[0m       output_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_types,\n\u001b[0;32m    783\u001b[0m       output_shapes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_shapes)\n\u001b[0;32m    785\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    786\u001b[0m     \u001b[39m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_spec\u001b[39m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32md:\\Work\\movie-recommendation-system\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3038\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   3036\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   3037\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3038\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   3039\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mIteratorGetNext\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, iterator, \u001b[39m\"\u001b[39;49m\u001b[39moutput_types\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_types,\n\u001b[0;32m   3040\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39moutput_shapes\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_shapes)\n\u001b[0;32m   3041\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   3042\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Shuffle the data\n",
    "shuffled = ratings.shuffle(200000, reshuffle_each_iteration=False)\n",
    "\n",
    "# Split train test\n",
    "train = shuffled.take(160000)\n",
    "test = shuffled.skip(160000).take(40000)\n",
    "\n",
    "# Get unique movie title and user id lists to be used as vocabulary\n",
    "movie_titles = ratings.batch(1000000).map(lambda x: x['title'])\n",
    "user_ids = ratings.batch(1000000).map(lambda x: x['title'])\n",
    "\n",
    "uniq_movie_title = np.unique(np.concatenate(list(movie_titles)))\n",
    "uniq_user_id = np.unique(np.concatenate(list(user_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ranking model\n",
    "class RankingModel(tf.keras.Model):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        embedding_dims = 32\n",
    "\n",
    "        # User embedding model        \n",
    "        self.movie_embedding = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.StringLookup(vocabulary=uniq_movie_title),\n",
    "            tf.keras.layers.Embedding(len(uniq_movie_title) + 1, embedding_dims)\n",
    "        ])\n",
    "\n",
    "        # Movie embedding model\n",
    "        self.user_embedding = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.StringLookup(vocabulary=uniq_user_id),\n",
    "            tf.keras.layers.Embedding(len(uniq_user_id) + 1, embedding_dims)\n",
    "        ])\n",
    "\n",
    "        # Rating prediction model\n",
    "        self.rating = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(256, 'relu'),\n",
    "            tf.keras.layers.Dense(64, 'relu'),\n",
    "            tf.keras.layers.Dense(1) # This will output the predicted rating\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_id, movie_title = inputs\n",
    "\n",
    "        user_embedding = self.user_embedding(user_id)\n",
    "        movie_embedding = self.movie_embedding(movie_title)\n",
    "\n",
    "        return self.rating(tf.concat([user_embedding, movie_embedding], axis=1)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define movie recommender model\n",
    "class MovieRecModel(tfrs.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ranking_model: tf.keras.Model = RankingModel()\n",
    "        self.task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
    "            loss=tf.keras.losses.MeanSquaredError(),\n",
    "            metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "        )\n",
    "\n",
    "    def call(self, inputs: Dict[Text, tf.Tensor]):\n",
    "        return self.ranking_model((inputs['userId'], inputs['title']))\n",
    "        \n",
    "\n",
    "    def compute_loss(self, inputs: Dict[Text, tf.Tensor], training: bool = False) -> tf.Tensor:\n",
    "        rating_prediction = self.ranking_model((inputs['userId'], inputs['title']))\n",
    "\n",
    "        return self.task(labels=inputs['rating'], predictions=rating_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "16/16 [==============================] - 4s 64ms/step - root_mean_squared_error: 0.9761 - loss: 0.8989 - regularization_loss: 0.0000e+00 - total_loss: 0.8989\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 1s 40ms/step - root_mean_squared_error: 0.1180 - loss: 0.0133 - regularization_loss: 0.0000e+00 - total_loss: 0.0133\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 1s 46ms/step - root_mean_squared_error: 0.0295 - loss: 8.3829e-04 - regularization_loss: 0.0000e+00 - total_loss: 8.3829e-04\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 1s 57ms/step - root_mean_squared_error: 0.0166 - loss: 2.7569e-04 - regularization_loss: 0.0000e+00 - total_loss: 2.7569e-04\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 1s 53ms/step - root_mean_squared_error: 0.0154 - loss: 2.3832e-04 - regularization_loss: 0.0000e+00 - total_loss: 2.3832e-04\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 1s 58ms/step - root_mean_squared_error: 0.0150 - loss: 2.2382e-04 - regularization_loss: 0.0000e+00 - total_loss: 2.2382e-04\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 1s 43ms/step - root_mean_squared_error: 0.0145 - loss: 2.1150e-04 - regularization_loss: 0.0000e+00 - total_loss: 2.1150e-04\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 1s 45ms/step - root_mean_squared_error: 0.0142 - loss: 2.0022e-04 - regularization_loss: 0.0000e+00 - total_loss: 2.0022e-04\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 1s 55ms/step - root_mean_squared_error: 0.0138 - loss: 1.8975e-04 - regularization_loss: 0.0000e+00 - total_loss: 1.8975e-04\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 1s 43ms/step - root_mean_squared_error: 0.0134 - loss: 1.8000e-04 - regularization_loss: 0.0000e+00 - total_loss: 1.8000e-04\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 1s 44ms/step - root_mean_squared_error: 0.0131 - loss: 1.7092e-04 - regularization_loss: 0.0000e+00 - total_loss: 1.7092e-04\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 1s 44ms/step - root_mean_squared_error: 0.0127 - loss: 1.6242e-04 - regularization_loss: 0.0000e+00 - total_loss: 1.6242e-04\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 1s 55ms/step - root_mean_squared_error: 0.0124 - loss: 1.5447e-04 - regularization_loss: 0.0000e+00 - total_loss: 1.5447e-04\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 1s 55ms/step - root_mean_squared_error: 0.0121 - loss: 1.4702e-04 - regularization_loss: 0.0000e+00 - total_loss: 1.4702e-04\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 1s 44ms/step - root_mean_squared_error: 0.0118 - loss: 1.4002e-04 - regularization_loss: 0.0000e+00 - total_loss: 1.4002e-04\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 1s 43ms/step - root_mean_squared_error: 0.0116 - loss: 1.3344e-04 - regularization_loss: 0.0000e+00 - total_loss: 1.3344e-04\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 1s 44ms/step - root_mean_squared_error: 0.0113 - loss: 1.2724e-04 - regularization_loss: 0.0000e+00 - total_loss: 1.2724e-04\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 1s 44ms/step - root_mean_squared_error: 0.0110 - loss: 1.2140e-04 - regularization_loss: 0.0000e+00 - total_loss: 1.2140e-04\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 1s 42ms/step - root_mean_squared_error: 0.0108 - loss: 1.1589e-04 - regularization_loss: 0.0000e+00 - total_loss: 1.1589e-04\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 1s 45ms/step - root_mean_squared_error: 0.0105 - loss: 1.1066e-04 - regularization_loss: 0.0000e+00 - total_loss: 1.1066e-04\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 1s 43ms/step - root_mean_squared_error: 0.0103 - loss: 1.0573e-04 - regularization_loss: 0.0000e+00 - total_loss: 1.0573e-04\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 1s 43ms/step - root_mean_squared_error: 0.0101 - loss: 1.0106e-04 - regularization_loss: 0.0000e+00 - total_loss: 1.0106e-04\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 1s 41ms/step - root_mean_squared_error: 0.0098 - loss: 9.6643e-05 - regularization_loss: 0.0000e+00 - total_loss: 9.6643e-05\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 1s 44ms/step - root_mean_squared_error: 0.0096 - loss: 9.2455e-05 - regularization_loss: 0.0000e+00 - total_loss: 9.2455e-05\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 1s 46ms/step - root_mean_squared_error: 0.0094 - loss: 8.8488e-05 - regularization_loss: 0.0000e+00 - total_loss: 8.8488e-05\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 1s 42ms/step - root_mean_squared_error: 0.0092 - loss: 8.4721e-05 - regularization_loss: 0.0000e+00 - total_loss: 8.4721e-05\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 1s 42ms/step - root_mean_squared_error: 0.0090 - loss: 8.1142e-05 - regularization_loss: 0.0000e+00 - total_loss: 8.1142e-05\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 1s 48ms/step - root_mean_squared_error: 0.0088 - loss: 7.7745e-05 - regularization_loss: 0.0000e+00 - total_loss: 7.7745e-05\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 1s 43ms/step - root_mean_squared_error: 0.0086 - loss: 7.4513e-05 - regularization_loss: 0.0000e+00 - total_loss: 7.4513e-05\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 1s 43ms/step - root_mean_squared_error: 0.0085 - loss: 7.1440e-05 - regularization_loss: 0.0000e+00 - total_loss: 7.1440e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f4f0426a60>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model = MovieRecModel()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "\n",
    "# Cache the dataset\n",
    "cached_train = train.batch(10000).cache()\n",
    "cached_test = test.batch(5000).cache()\n",
    "\n",
    "# Fit the model to train dataset\n",
    "model.fit(cached_train, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 3s 25ms/step - root_mean_squared_error: 0.0084 - loss: 7.0474e-05 - regularization_loss: 0.0000e+00 - total_loss: 7.0474e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'root_mean_squared_error': 0.008395403623580933,\n",
       " 'loss': 7.040308992145583e-05,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 7.040308992145583e-05}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model\n",
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.4916122]\n",
      "[2.4952605]\n",
      "[2.4957926]\n"
     ]
    }
   ],
   "source": [
    "# Inference testing those movies for user 20\n",
    "rating_result = {}\n",
    "movies_to_test = ['Toy Story (1995)', 'Jumanji (1995)', 'Star Wars: The Last Jedi (2017)']\n",
    "for title in movies_to_test:\n",
    "    rating_result[title] =\\\n",
    "        model({'userId': np.array(['50']),\n",
    "               'title': np.array([title])})\n",
    "\n",
    "for x in rating_result.values():\n",
    "    print(x.numpy()[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
